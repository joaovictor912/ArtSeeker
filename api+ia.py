# -*- coding: utf-8 -*-
"""API+IA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g4_tR9IMZljrdlDtFEiC272BItIlAixj

# Identificando, a partir da API, a quantidade de obras no acervo.
"""

import requests

def main():
    url = "https://api.artic.edu/api/v1/artworks"
    response = requests.get(url)

    if response.status_code != 200:
        print("Erro ao acessar a API:", response.status_code)
        return

    content = response.json()

    total_obras = content.get("pagination", {}).get("total", "Desconhecido")
    print(f"Total de obras no acervo: {total_obras}\n")

main()

"""# Código para busca exata"""

def main():
    termo = input("Digite o nome da obra, autor ou data: ").strip()

    url = "https://api.artic.edu/api/v1/artworks/search"
    params = {
        "q": termo,
        "limit": 25
    }

    response = requests.get(url, params=params)

    if response.status_code != 200:
        print("Erro ao acessar a API:", response.status_code)
        return

    content = response.json()
    resultados = content.get("data", [])

    encontrados = []

    for obra in resultados:
        obra_id = obra.get("id")
        if not obra_id:
            continue

        #Obtendo dados completos da obra
        url_detalhes = f"https://api.artic.edu/api/v1/artworks/{obra_id}"
        resposta_detalhes = requests.get(url_detalhes)

        if resposta_detalhes.status_code != 200:
            continue

        dados_completos = resposta_detalhes.json().get("data", {})

        titulo = dados_completos.get("title", "").lower()
        artista = dados_completos.get("artist_title", "").lower()
        data = dados_completos.get("date_display", "").lower()

        # Busca exata
        if termo.lower() in titulo or termo.lower() in artista or termo.lower() in data:
            encontrados.append({
                "title": dados_completos.get("title", "Sem título"),
                "artist": dados_completos.get("artist_title", "Autor desconhecido"),
                "date": dados_completos.get("date_display", "Data desconhecida")
            })

    if not encontrados:
        print("Nenhuma obra encontrada.")
    else:
        print(f"\nObra(s) encontrada(s) para'{termo}':")
        for obra in encontrados:

            print(f"Título: {obra['title']}")
            print(f"Autor: {obra['artist']}")
            print(f"Data: {obra['date']}")
            print("-" * 40)

main()

"""# Código para busca aproximada utilizando a biblioteca rapidfuzz/ fuzz"""

!pip install rapidfuzz #INSTALANDO BIBLIOTECA

from rapidfuzz import fuzz

def main():
    termo = input("Digite o nome da obra, autor ou data: ").strip()

    url_busca = "https://api.artic.edu/api/v1/artworks/search"
    params = {
        "q": termo,
        "limit": 25
    }

    response = requests.get(url_busca, params=params)

    if response.status_code != 200:
        print("Erro ao acessar a API:", response.status_code)
        return

    content = response.json()
    resultados = content.get("data", [])

    encontrados = []

    #ID da obra buscada
    for obra in resultados:
        obra_id = obra.get("id")
        if obra_id is None:
            continue

        url_detalhes = f"https://api.artic.edu/api/v1/artworks/{obra_id}"
        resposta_detalhes = requests.get(url_detalhes)

        if resposta_detalhes.status_code != 200:
            continue

        detalhes = resposta_detalhes.json().get("data", {})

        titulo = detalhes.get("title", "")
        artista = detalhes.get("artist_title", "")
        data = detalhes.get("date_display", "")

        # Fuzzy match nos campos reais para os dados completos
        if (fuzz.partial_ratio(termo.lower(), titulo.lower()) > 70 or
            fuzz.partial_ratio(termo.lower(), artista.lower()) > 70 or
            fuzz.partial_ratio(termo.lower(), data.lower()) > 70):

            encontrados.append({
                "title": titulo or "Sem título",
                "artist": artista or "Autor desconhecido",
                "date": data or "Data desconhecida"
            })

    total = len(encontrados)

    if not encontrados:
        print("Nenhuma obra encontrada com esse critério.")
    else:
        print(f"\n{total} obra(s) semelhante(s) ao termo '{termo}':")
        for obra in encontrados:
            print(f"\nTítulo: {obra['title']}")
            print(f"Autor: {obra['artist']}")
            print(f"Data: {obra['date']}")
            print("-" * 40)

main()



"""# IA para encontrar obra a partir de prompt prévio

1° Passo: coletar obras da API → título, artista, descrição
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile config.py
# # Ficheiro: config.py
# # Central de configurações para o projeto de busca de obras de arte.
# ARQUIVO_CACHE_OBRAS = "obras_pinturas_detalhadas.json"
# 
# ARQUIVO_SAIDA_JSON = "obras_com_embeddings.json"
# ARQUIVO_SAIDA_EMBEDDINGS = "embeddings.npy"
# ARQUIVO_INDICE_FAISS = "obras.index"
# 
# #CONFIGURAÇÕES DA COLETA DE DADO
# API_URL_BUSCA = "https://api.artic.edu/api/v1/artworks/search"
# MAX_OBRAS_COLETA = 10000
# INTERVALO_ANOS_BUSCA = 50
# ANO_INICIAL_BUSCA = 1400
# ANO_FINAL_BUSCA = 2024
# LIMITE_POR_PAGINA_API = 100
# MAX_TOTAL_RESULTADOS_PER_QUERY = 10000
# CACHE_FILE_COLETA = ARQUIVO_CACHE_OBRAS
# 
# #MODELOS (PASSOS 1 e 2)
# MODELO_ST = 'intfloat/multilingual-e5-large'
# MODELO_CE = 'cross-encoder/ms-marco-MiniLM-L-6-v2'
# 
# #PARÂMETROS DE PROCESSAMENTO E BUSCA
# ARQUIVO_OBRAS_DETALHADAS = ARQUIVO_CACHE_OBRAS
# LIMITE_OBRAS_PROCESSAMENTO = None
# BATCH_SIZE = 32
# TOP_K_CANDIDATOS = 25
# TOP_K_FINAL = 10
# LIMIAR_SIMILARIDADE = 0.40

!pip install requests beautifulsoup4 tenacity sentence-transformers faiss-cpu scikit-learn tqdm

import os
import json
import requests
from typing import List, Dict, Any, Set
from bs4 import BeautifulSoup
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type, RetryError

# Importa as configurações que criámos na Célula 1
import config

def coletar_obras_de_arte():
    """
    Função principal para coletar dados da API, usando cache e segmentação por data.
    Utiliza as configurações definidas no ficheiro config.py.
    """
    obras_coletadas = []
    cached_ids = set()

    # Carrega o cache se o ficheiro existir
    if os.path.exists(config.CACHE_FILE_COLETA):
        try:
            with open(config.CACHE_FILE_COLETA, 'r', encoding='utf-8') as f:
                obras_coletadas = json.load(f)
            cached_ids = {obra['id'] for obra in obras_coletadas if 'id' in obra}
            print(f"Cache carregado com sucesso: {len(obras_coletadas)} obras encontradas.")
        except json.JSONDecodeError:
            print("AVISO: Ficheiro de cache corrompido. A iniciar uma nova coleta.")
            obras_coletadas, cached_ids = [], set()

    if len(obras_coletadas) >= config.MAX_OBRAS_COLETA:
        print(f"O número máximo de obras ({config.MAX_OBRAS_COLETA}) já foi atingido no cache.")
        return

    # Campos desejados da API
    campos_desejados = [
        "id", "title", "artist_title", "date_display", "term_titles",
        "medium_display", "style_titles", "subject_titles", "artwork_type_title", "description"
    ]

    session = requests.Session()
    session.headers.update({'User-Agent': 'ArtProjectBot/1.0 (Colab script for academic project)'})

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type(requests.exceptions.RequestException)
    )
    def buscar_pagina(payload: Dict[str, Any]) -> Dict[str, Any]:
        """Função aninhada para fazer o pedido à API com novas tentativas (retries)."""
        response = session.post(config.API_URL_BUSCA, json=payload, timeout=30)
        response.raise_for_status()
        return response.json()

    # Loop principal segmentado por datas
    for ano_inicio in range(config.ANO_INICIAL_BUSCA, config.ANO_FINAL_BUSCA, config.INTERVALO_ANOS_BUSCA):
        if len(obras_coletadas) >= config.MAX_OBRAS_COLETA:
            break

        ano_fim = min(ano_inicio + config.INTERVALO_ANOS_BUSCA - 1, config.ANO_FINAL_BUSCA)
        print(f"\n--- Coletando obras do período: {ano_inicio} - {ano_fim} ---")

        max_paginas_por_intervalo = config.MAX_TOTAL_RESULTADOS_PER_QUERY // config.LIMITE_POR_PAGINA_API

        for pagina_atual in range(1, max_paginas_por_intervalo + 1):
            if len(obras_coletadas) >= config.MAX_OBRAS_COLETA:
                break

            payload = {
                'page': pagina_atual,
                'limit': config.LIMITE_POR_PAGINA_API,
                'fields': campos_desejados,
                'query': {
                    "bool": {
                        "must": [
                            {"term": {"classification_titles.keyword": "painting"}},
                            {"range": {"date_start": {"gte": ano_inicio, "lte": ano_fim}}}
                        ]
                    }
                }
            }

            try:
                data = buscar_pagina(payload)
                novas_obras = data.get('data', [])
                if not novas_obras:
                    print(f"Nenhum resultado encontrado para este período na página {pagina_atual}. Passando para o próximo intervalo de datas.")
                    break

                obras_processadas_pagina = 0
                for obra in novas_obras:
                    if 'id' in obra and obra['id'] not in cached_ids:
                        descricao_html = obra.get('description')
                        texto_limpo = BeautifulSoup(descricao_html, 'html.parser').get_text(' ', strip=True) if descricao_html else 'Não disponível'

                        #Harmoniza o dicionário da obra para ser compatível com o Passo 1
                        obra_limpa = {
                            'id': obra['id'],
                            'title': obra.get('title', 'Sem título'),
                            'artist': obra.get('artist_title', 'Desconhecido'), # Harmonizado
                            'date': obra.get('date_display', 'Data desconhecida'), # Harmonizado
                            'artwork_type_title': obra.get('artwork_type_title'),
                            'medium_display': obra.get('medium_display'),
                            'style_titles': obra.get('style_titles', []),
                            'term_titles': obra.get('term_titles', []),
                            'subject_titles': obra.get('subject_titles', []),
                            'description_api': texto_limpo # Harmonizado
                        }

                        obras_coletadas.append(obra_limpa)
                        cached_ids.add(obra['id'])
                        obras_processadas_pagina += 1

                print(f"Página {pagina_atual}: {obras_processadas_pagina} novas obras adicionadas. Total: {len(obras_coletadas)}")

            except RetryError as e:
                print(f"ERRO: Falha na conexão após várias tentativas: {e}. A passar para o próximo intervalo.")
                break
            except Exception as e:
                print(f"ERRO INESPERADO: {type(e).__name__} - {e}. A interromper a coleta deste intervalo.")
                break

    # Salva o resultado final no ficheiro de cache
    try:
        with open(config.CACHE_FILE_COLETA, 'w', encoding='utf-8') as f:
            json.dump(obras_coletadas[:config.MAX_OBRAS_COLETA], f, ensure_ascii=False, indent=4)
        print(f"\nColeta finalizada. {len(obras_coletadas)} obras foram salvas em '{config.CACHE_FILE_COLETA}'.")
    except IOError as e:
        print(f"ERRO CRÍTICO: Não foi possível salvar o ficheiro de cache: {e}")


print("coleta")
coletar_obras_de_arte()

"""2°Passo: descrição unificada das obras (embeddings); Embeddings uma técnica utilizada em processamento de linguagem natural para transformar palavras ou frases em vetores de números. Isso permite calcularmos diferenças entre textos em operações matemáticas (facilitação de busca semântica).

Funções principais:

- Lê o JSON com os dados detalhados das obras.
- Gera uma descrição unificada para cada obra (concatenando artista, data, estilo, etc.).
- Codifica essas descrições com o modelo SentenceTransformer.
- Normaliza os embeddings e salva os resultados em:
    - obras_com_embeddings.json
    - embeddings.npy

Modelo sentence transformer (ST) utilizado intfloat/multilingual-e5-large, gera embeddings vetoriais de textos (transforma frases em vetores de números). Utilizado para gerar  embeddings de texto em um idioma diferente do inglês

Modelo cross encoder (CE) utilizado cross-encoder/ms-marco-MiniLM-L-6-v2, recebe dois textos de entrada juntos (ex: consulta do usuário + descrição da obra) e calcula um score de relevância diretamente. Primeiro filtra as top-N obras com embeddings (st), depois passa essas top-N no cross encoder, que vai dar uma pontuação mais precisa.
"""

import json
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any, Optional
from tqdm import tqdm
import config

def carregar_obras_detalhadas(caminho_arquivo: str, limite: Optional[int]) -> List[Dict[str, Any]]:
    """Carrega os dados das obras a partir do ficheiro JSON gerado no Passo 0."""
    try:
        with open(caminho_arquivo, 'r', encoding='utf-8') as f:
            obras_data = json.load(f)
        print(f"Carregadas {len(obras_data)} obras de '{caminho_arquivo}'.")
        if limite:
            print(f"Aplicando limite de {limite} obras para processamento.")
            return obras_data[:limite]
        return obras_data
    except FileNotFoundError:
        print(f"ERRO: Arquivo '{caminho_arquivo}' não encontrado. Executaste o Passo 0 (Coleta de Dados) primeiro?")
        return []
    except json.JSONDecodeError:
        print(f"ERRO: O arquivo '{caminho_arquivo}' contém um JSON inválido.")
        return []

def gerar_descricoes_unificadas(obras: List[Dict[str, Any]]) -> (List[str], List[Dict[str, Any]]):
    """Gera uma descrição textual detalhada e otimizada para cada obra."""
    descricoes_para_embedding = []
    obras_com_descricao_final = []

    print("Gerando descrições unificadas para as obras...")
    for obra in tqdm(obras, desc="Gerando Descrições"):
        estilo = ", ".join(obra.get('style_titles', [])) or "N/A"

        descricao = f"A obra '{obra.get('title', 'Sem título')}'"
        if obra.get('artist') and obra['artist'].lower() != 'desconhecido':
            descricao += f", criada por {obra['artist']}"
        if obra.get('date') and obra['date'].lower() != 'data desconhecida':
            descricao += f", datada de {obra['date']}"
        descricao += "."

        if estilo != 'N/A':
            descricao += f" Estilo(s): {estilo}."


        entrada_modelo = f"passage: {descricao}"

        descricoes_para_embedding.append(entrada_modelo)
        obra_copy = obra.copy()
        obra_copy['descricao_para_embedding'] = descricao
        obras_com_descricao_final.append(obra_copy)

    return descricoes_para_embedding, obras_com_descricao_final

def gerar_e_salvar_embeddings(
    model: SentenceTransformer,
    descricoes: List[str],
    caminho_saida: str
) -> np.ndarray:
    """Gera os embeddings usando o modelo e salva-os num ficheiro .npy."""
    print(f"Gerando embeddings com o modelo '{config.MODELO_ST}'...")
    embeddings = model.encode(
        descricoes,
        batch_size=config.BATCH_SIZE,
        show_progress_bar=True,
        normalize_embeddings=True
    )
    # Converte para float32, otimizado para FAISS
    embeddings = embeddings.astype(np.float32)

    print(f"Salvando {len(embeddings)} embeddings em '{caminho_saida}'...")
    np.save(caminho_saida, embeddings)
    return embeddings

def criar_e_salvar_indice_faiss(embeddings: np.ndarray, caminho_saida: str):
    """Cria um índice FAISS para busca rápida e salva-o em disco."""
    if embeddings.size == 0:
        print("Não há embeddings para criar o índice.")
        return

    dimensao = embeddings.shape[1]
    #IndexFlatIP  vetores estão normalizados (produto interno = similaridade de cosseno)
    index = faiss.IndexFlatIP(dimensao)
    index.add(embeddings)

    print(f"Índice FAISS criado com {index.ntotal} vetores.")
    print(f"Salvando índice FAISS em '{caminho_saida}'...")
    faiss.write_index(index, caminho_saida)

def pipeline_de_processamento():
    """Função principal que orquestra todo o pipeline de pré-processamento."""
    print("--- INICIANDO PASSO 1: GERAÇÃO DE EMBEDDINGS E ÍNDICE ---")

    # Carrega os dados coletados no passo anterior, usando os nomes de ficheiro do config
    obras = carregar_obras_detalhadas(config.ARQUIVO_OBRAS_DETALHADAS, config.LIMITE_OBRAS_PROCESSAMENTO)
    if not obras:
        print("Nenhuma obra para processar. Encerrando o Passo 1.")
        return

    # Carrega o modelo de embedding
    model = SentenceTransformer(config.MODELO_ST)

    descricoes, obras_com_descricao = gerar_descricoes_unificadas(obras)

    with open(config.ARQUIVO_SAIDA_JSON, 'w', encoding='utf-8') as f:
        json.dump(obras_com_descricao, f, ensure_ascii=False, indent=4)
    print(f"Obras com descrições salvas em '{config.ARQUIVO_SAIDA_JSON}'.")

    embeddings = gerar_e_salvar_embeddings(model, descricoes, config.ARQUIVO_SAIDA_EMBEDDINGS)

    criar_e_salvar_indice_faiss(embeddings, config.ARQUIVO_INDICE_FAISS)

    print("Os ficheiros de embeddings, o JSON enriquecido e o índice FAISS estão prontos para a busca.")

pipeline_de_processamento()

"""3°Busca -2°VERSÃO (Mais eficiente, robusta e mais preparada para o mundo real)

Funções principais:
- Carrega modelo e embeddings salvos.
- Cria índice FAISS para busca eficiente (IndexFlatIP com vetores normalizados) = faz a busca usando similaridade de cossenos.
- Codifica uma consulta em embedding e compara com os embeddings existentes.
- Aplica filtros e exibe resultados com base na similaridade.
- Identifica baixa similaridade


"""

import json
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer, CrossEncoder
from typing import List, Dict, Any, Tuple
import config

def carregar_recursos() -> Tuple:
    """
    Carrega todos os recursos necessários de uma só vez: modelos,
    dados das obras e o índice FAISS pré-construído.
    """
    print("Carregando recursos para a busca...")
    try:

        modelo_st = SentenceTransformer(config.MODELO_ST)
        modelo_ce = CrossEncoder(config.MODELO_CE)
        with open(config.ARQUIVO_SAIDA_JSON, "r", encoding="utf-8") as f:
            obras = json.load(f)
        index = faiss.read_index(config.ARQUIVO_INDICE_FAISS)

        print(f"Recursos carregados com sucesso: {len(obras)} obras, {index.ntotal} vetores no índice.")
        return modelo_st, modelo_ce, obras, index
    except FileNotFoundError as e:
        print(f"ERRO: Não foi possível carregar um recurso essencial: {e}")
        print("Por favor, execute as células dos Passos 0 e 1 primeiro.")
        return None, None, None, None

def buscar_candidatos(query: str, modelo_st: SentenceTransformer, index: faiss.Index) -> Tuple[List[float], List[int]]:
    """Codifica a query e busca no FAISS para obter os candidatos iniciais (etapa de recuperação)."""
    # Adiciona o prefixo "query: " específico do modelo E5
    query_com_prefixo = 'query: ' + query
    emb_query = modelo_st.encode(
        [query_com_prefixo],
        normalize_embeddings=True
    ).astype(np.float32)

    # Realiza a busca no índice FAISS
    # D = Distâncias (scores de similaridade), I = Índices das obras
    D, I = index.search(emb_query, config.TOP_K_CANDIDATOS)
    return D[0], I[0]

def reordenar_resultados(query: str, obras: List[Dict], indices: List[int], modelo_ce: CrossEncoder) -> List[Tuple[float, int]]:
    """Usa o CrossEncoder para reordenar os candidatos e obter resultados mais precisos (etapa de reordenação)."""
    # Cria os pares (query, descrição) para o CrossEncoder avaliar
    pares = [(query, obras[idx]['descricao_para_embedding']) for idx in indices]

    print(f"Reordenando {len(pares)} candidatos com o Cross-Encoder para maior precisão...")
    scores = modelo_ce.predict(pares, show_progress_bar=False)

    # Combina os scores com os índices originais e ordena pelo novo score
    resultados_finais = sorted(zip(scores, indices), key=lambda x: x[0], reverse=True)
    return resultados_finais

def exibir_resultados(query: str, resultados: List[Tuple[float, int]], obras: List[Dict]):
    """Formata e exibe os resultados finais da busca de forma legível."""
    print(f"\n--- Top {config.TOP_K_FINAL} Resultados para a consulta: '{query}' ---\n")
    for i, (score, idx) in enumerate(resultados[:config.TOP_K_FINAL]):
        obra = obras[idx]
        print("-" * 60)
        print(f"RANK: {i + 1} | SCORE DE RELEVÂNCIA: {score:.4f}")
        print(f"Título: {obra.get('title', 'Sem título')}")
        print(f"Artista: {obra.get('artist', 'Desconhecido')}")
        print(f"Data: {obra.get('date', 'Desconhecida')}")
        print(f"Descrição: {obra.get('descricao_para_embedding', '')[:300]}...")
        print("-" * 60)
        print()

def interface_de_busca():
    """Função principal que gerencia a interface de busca interativa."""
    recursos = carregar_recursos()
    if any(r is None for r in recursos):
        return

    modelo_st, modelo_ce, obras, index = recursos

    print("\n Interface de Busca Pronta")
    print("Digite a sua consulta (ex: 'pintura cubista com formas geométricas') ou 'sair' para terminar.")

    while True:
        query = input("Consulta> ").strip()
        if query.lower() == 'sair':
            print("Encerrando a busca. Até logo!")
            break
        if not query:
            continue

       ##Recuperação rápida de candidatos com FAISS
        scores_st, indices_st = buscar_candidatos(query, modelo_st, index)

        #Filtra candidatos com baixa similaridade inicial para não sobrecarregar o Cross-Encoder
        indices_filtrados = [idx for idx, score in zip(indices_st, scores_st) if score >= config.LIMIAR_SIMILARIDADE]

        if not indices_filtrados:
            print("Nenhuma obra relevante encontrada. Tente uma busca com outros termos.\n")
            continue

        #  Reordenação dos melhores candidatos para máxima precisão
        resultados_reordenados = reordenar_resultados(query, obras, indices_filtrados, modelo_ce)

        #Exibição dos resultados finais
        exibir_resultados(query, resultados_reordenados, obras)

interface_de_busca()